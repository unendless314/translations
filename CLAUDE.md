# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a modular subtitle translation pipeline that processes SRT files through a YAML-based workflow. When a download arrives as YouTube `.sbv`, place it under `sbv/` and run `tools/sbv_to_srt.py` to produce the SRT that feeds the standard sequence. The system uses LLMs for translation with structured context (topics, terminology, guidelines) to ensure consistent, high-quality subtitle translations.

## Core Data Architecture

The project uses a **three-layer file structure**:
- `sbv/` - Optional holding area for raw `.sbv` captions before conversion
- `input/<episode>/` - Original SRT files
- `data/<episode>/` - Working YAML/Markdown files (main.yaml, topics.json, terminology.yaml, guidelines.md)
- `data/<episode>/drafts/` - Topic-based translation work files (topic_01.md, topic_02.md, etc.)
- `output/<episode>/` - Exported results (SRT/Markdown/reports)

### Primary Data Files (per episode)

**`data/<episode>/main.yaml`** - The central data file containing:
- All parsed SRT segments with timecodes
- Translation results and status tracking
- Segment-level metadata (topic_id, speaker_group, music tags, etc.)

**`data/<episode>/topics.json`** - Thematic structure:
- Topics with segment ranges (`segment_start`, `segment_end`)
- Per-topic summaries and keywords
- Global episode summary for context

**`data/<episode>/terminology.yaml`** - Multi-sense term definitions:
- Terms can have multiple senses (e.g., "channel" as broadcast vs. spiritual)
- Each sense includes preferred translation, definition, applicable segments/topics
- Used to ensure consistent terminology across translations

**`data/<episode>/guidelines.md`** - Translation style guide:
- Tone and voice requirements (e.g., contemplative, spiritual)
- Special formatting rules
- Episode-specific translation instructions
- Loaded as system prompt for translation models

**`data/<episode>/drafts/topic_XX.md`** - Topic-based translation work files:
- Generated by `prepare_topic_drafts.py` from main.yaml
- Each file covers one topic with its segment range
- Contains source text with empty JSON translation fields: `{"text": "", "confidence": "", "notes": ""}`
- Translators directly edit these files to fill in translations
- Processed by `backfill_translations.py` to update main.yaml

### Configuration Files

`configs/default.yaml` æä¾›æ‰€æœ‰å…±ç”¨è·¯å¾‘èˆ‡æ¨¡å‹è¨­å®šï¼Œ`configs/<episode>.yaml` é€šå¸¸åªéœ€è¦ï¼š
```yaml
episode_id: S01-E12
# input:
#   srt: input/S01-E12/custom_file.srt  # è‹¥è³‡æ–™å¤¾ä¸­æœ‰å¤šå€‹ SRT æ‰éœ€è¦è¦†å¯«
```
è…³æœ¬æœƒè‡ªå‹•æ–¼ `input/<episode>/` ä¸­å°‹æ‰¾å”¯ä¸€ `.srt` æª”æ¡ˆï¼Œè¼¸å‡ºèˆ‡æ—¥èªŒå‰‡è½åœ¨ `data/<episode>/...`ã€`logs/<episode>/workflow.log`ã€‚

## Key Processing Rules

### Status Tracking
Each segment has `translation.status`:
- `pending` - Not yet translated
- `in_progress` - Currently being processed
- `completed` - Translation finished
- `needs_review` - Flagged for manual review
- `approved` - Reviewed and approved

### Resume Support
Tools should check `translation.status` to skip already-completed segments, enabling interrupted workflow continuation.

## Key Processing Rules

### SRT Parsing and Segment Merging
When converting SRT to main.yaml:
- **Sentence completeness priority**: MUST merge segments until sentence has terminal punctuation (`.!?â€¦`)
- **Stop merging**: Once sentence is complete, stop if next entry starts with uppercase (new sentence)
- **Safety limit**: Maximum 10 SRT entries per segment to prevent pathological cases
- **Speaker detection**: `>>` prefix indicates speaker change, increment `speaker_group`
- **Music/sound tags**: Preserve `[MUSIC]` tags in `source_text`, let AI translate them
- **Timecode preservation**: Keep original SRT format (`HH:MM:SS,mmm`)
- **Source tracking**: Record original SRT indices in `metadata.source_entries`

### Topics Generation Flow
1. Export `main.yaml` to JSON with segment markers (`segment_id`, `speaker_group`, `source_text`)
2. Feed to large-context LLM (e.g., Gemini 3 Pro) using `prompts/topic_analysis_system.txt`
3. Parse LLM output to extract segment ranges, summaries, keywords
4. Generate `topics.json` structure with validation (no gaps, no overlaps, sequential ranges)

### Terminology Mapping (Candidate Generation)
The `terminology_mapper.py` tool generates `terminology_candidates.yaml`:
- Scans `main.yaml` to find all occurrences of terms from `terminology_template.yaml` and `topics.json` keywords
- For each term, records all matching segments with `segment_id`, `sources` (template/topic), and `source_text`
- Outputs `occurrences` arrays (not yet classified into senses)
- This is the **input** for the manual/AI classification step that produces `terminology.yaml`

The subsequent classification step (manual or via Claude Code):
- Reviews each occurrence's `source_text` context
- Assigns each `segment_id` to the appropriate sense based on semantic meaning
- Produces `terminology.yaml` with `segments` arrays (classified and sense-specific)
- Performs multi-sense disambiguation based on context

## Tool Design Principles

### Command-Line Interface
All tools should accept:
- `--config configs/<episode>.yaml` as primary configuration
- Optional CLI overrides for specific parameters
- `--force` to overwrite existing outputs
- `--resume` to continue from last checkpoint (where applicable)

### Error Handling
- Log unparseable timecodes but continue processing
- Mark problematic segments with `status: error` or `metadata.truncated: true`
- Never silently fail; provide actionable error messages
- Use non-zero exit codes for failures

### Incremental Writing
- Write results back to `main.yaml` after each batch
- Avoid accumulating large amounts in memory
- Support partial completion and recovery

## Project Architecture

### Directory Structure
```
src/                    # ğŸ†• Shared modules
â”œâ”€â”€ clients/           # LLM API clients
â”‚   â”œâ”€â”€ base_client.py      # Abstract base class
â”‚   â”œâ”€â”€ gemini_client.py    # Gemini (google-genai SDK)
â”‚   â”œâ”€â”€ openai_client.py    # OpenAI (planned)
â”‚   â””â”€â”€ anthropic_client.py # Anthropic (planned)
â”œâ”€â”€ models.py          # Data models (@dataclass)
â””â”€â”€ exceptions.py      # Custom exceptions

tools/                 # CLI tool scripts
â”œâ”€â”€ srt_to_main_yaml.py       âœ…
â”œâ”€â”€ main_yaml_to_json.py      âœ…
â”œâ”€â”€ topics_analysis_driver.py âœ…
â””â”€â”€ ...                       (planned)
```

## Development Commands

### Setup
```bash
# Install dependencies (includes google-genai 0.1.0+)
pip install -r requirements.txt

# Configure API keys
cp .env.example .env
# Edit .env and add your API keys:
# - GEMINI_API_KEY (recommended for topic analysis)
# - OPENAI_API_KEY (alternative)
```

### Run Tools
```bash
# Step 1: Convert SRT to main.yaml
# (Optional) If the source is SBV, run:
# PYTHONPATH=. python3 tools/sbv_to_srt.py --input sbv/captions.sbv --output input/S01-E12/source.srt
PYTHONPATH=. python3 tools/srt_to_main_yaml.py --config configs/S01-E12.yaml [--force] [--verbose]

# Step 2: Export segments to JSON
PYTHONPATH=. python3 tools/main_yaml_to_json.py --config configs/S01-E12.yaml [--pretty] [--verbose]

# Step 3: Generate topics.json (requires API key)
PYTHONPATH=. python3 tools/topics_analysis_driver.py --config configs/S01-E12.yaml [--dry-run] [--verbose]

# Step 4: Generate translation drafts (Markdown work files)
# This creates data/<episode>/drafts/topic_01.md, topic_02.md, etc.
# Each file contains source text with empty JSON translation fields
PYTHONPATH=. python3 tools/prepare_topic_drafts.py --config configs/S01-E12.yaml [--force] [--verbose]

# Step 5: Translate (manual editing or with Claude Code/LLM assistance)
# Work files: data/<episode>/drafts/topic_XX.md
# Edit each file to fill in JSON fields:
#   {"text": "ç¿»è­¯å…§å®¹", "confidence": "high/medium/low", "notes": "å‚™è¨»èªªæ˜"}
# Context files available for reference:
#   - data/<episode>/guidelines.md (translation style guide)
#   - data/<episode>/terminology.yaml (terminology reference)
#   - data/<episode>/topics.json (topic summaries and context)

# Step 5.5: QA - Fix Chinese punctuation (recommended after translation)
# Automatically corrects English punctuation to Chinese in translation text fields
# LLM translations often mix English commas (,) instead of Chinese (ï¼Œ)
PYTHONPATH=. python3 tools/fix_chinese_punctuation.py --config configs/S01-E12.yaml [--dry-run] [--verbose]
# Alternative: manually specify files
# PYTHONPATH=. python3 tools/fix_chinese_punctuation.py data/S01-E12/drafts/topic_*.md [--dry-run] [--verbose]

# Step 6: Backfill completed translations to main.yaml
# Reads completed topic_XX.md files and updates main.yaml with translations
PYTHONPATH=. python3 tools/backfill_translations.py --config configs/S01-E12.yaml [--dry-run] [--verbose]

# Step 7: Export translated SRT subtitles
# Converts main.yaml translations back to SRT format
PYTHONPATH=. python3 tools/export_srt.py --config configs/S01-E12.yaml [--no-speaker-hints] [--fail-on-missing] [--verbose]

# Step 8: Split long subtitle segments (optional but recommended)
# Intelligently splits long segments at punctuation marks for better readability
# IMPORTANT: This tool splits each segment ONCE per run. For very long segments (>100 chars),
# you may need to run it 2-3 times to achieve the target length.
# The tool will automatically report remaining long segments and suggest re-running if needed.

PYTHONPATH=. python3 tools/split_srt.py \
  -i output/S01-E12/S01-E12.zh-TW.srt \
  -o output/S01-E12/S01-E12.zh-TW.split.srt \
  --max-chars 35 \
  [--min-chars 10] \
  [--gap-ms 0] \
  [--verbose]

# If the tool reports remaining long segments, run it again on the output:
# PYTHONPATH=. python3 tools/split_srt.py \
#   -i output/S01-E12/S01-E12.zh-TW.split.srt \
#   -o output/S01-E12/S01-E12.zh-TW.split2.srt \
#   --max-chars 35

# Typical convergence:
# - Run 1: 115 long segments (max 140 chars) â†’ 51 long segments (max 71 chars)
# - Run 2: 51 long segments â†’ 9 long segments (max 47 chars)
# - Run 3: 9 long segments â†’ ~5 long segments (max 44 chars, usually acceptable)
```

## Implementation Status

### Currently Implemented
**Tools:**
- `tools/srt_to_main_yaml.py` - SRT parser with intelligent sentence merging âœ…
- `tools/main_yaml_to_json.py` - Export minimal segments for LLM analysis âœ…
- `tools/topics_analysis_driver.py` - LLM-based topic analysis âœ…
- `tools/terminology_mapper.py` - Produce terminology_candidates.yaml with per-term occurrences âœ…
- `tools/prepare_topic_drafts.py` - Generate topic-based translation work files (topic_XX.md) âœ…
- `tools/backfill_translations.py` - Read completed topic_XX.md files and update main.yaml with translations âœ…
- `tools/fix_chinese_punctuation.py` - QA tool to fix English punctuation in Chinese translations âœ…
- `tools/export_srt.py` - Convert main.yaml back to SRT format âœ…
- `tools/split_srt.py` - Split long SRT subtitles for better readability âœ…
- `tools/sbv_to_srt.py` - Convert YouTube SBV captions into standard SRT before entering the pipeline âœ…

**Shared Modules:**
- `src/clients/base_client.py` - Abstract LLM client interface âœ…
- `src/clients/gemini_client.py` - Gemini API (google-genai SDK 0.1.0+) âœ…
- `src/clients/openai_client.py` - OpenAI API (GPT-5, Responses API) âœ…
- `src/models.py` - Data models (APIResponse, TokenUsage) âœ…
- `src/exceptions.py` - Custom exceptions âœ…

**Configuration:**
- `configs/S01-E12.yaml` - Episode config with model settings âœ…
- `.env.example` - API key template âœ…
- `prompts/topic_analysis_system.txt` - Topic analysis prompt âœ…

**Documentation:**
- `docs/ARCHITECTURE.md` - Architectural design document âœ…
- `CLAUDE.md` - This file âœ…

### Planned Tools (see docs/TOOL_SPEC.md)
1. `terminology_classifier.py` - Assign occurrences to senses and write terminology.yaml
2. `translation_driver.py` - Orchestrate batch translation with model I/O (optional automated approach)
3. `qa_checker.py` - Unified QA tool runner (integrates multiple specialized QA tools when mature enough)
4. `export_markdown.py` - Generate human-readable translation reports

**Note on QA Strategy:** The project follows an incremental approach to QA automation. Individual QA tools (like `fix_chinese_punctuation.py`) are developed as specific, repeating issues are identified through manual QA across multiple episodes. Once 3-5 specialized QA tools are established, they may be integrated into a unified `qa_checker.py` runner.

## Translation Quality Checks

QA tools should validate:
- **Punctuation consistency** - Chinese translations must use Chinese punctuation (ï¼Œnot ,)
- Terminology consistency across segments
- Translation confidence scores
- Text length ratios (source vs. translation)
- Timecode integrity
- Status completeness (all segments translated)
- Segments with `metadata.truncated: true` should be flagged as `needs_review`

**Note:** LLM translations commonly mix English punctuation in Chinese text. Always run `fix_chinese_punctuation.py` after translation (Step 5.5) before backfilling to main.yaml.

## Important Notes

- **Episode ID is the primary key** - all file operations use this identifier
- **All documentation is in Traditional Chinese** - except this CLAUDE.md file and code comments
- **API Keys Required** - LLM tools need `.env` file with provider API keys (see `.env.example`)
- **Model Configuration** - Each episode config specifies model provider, name, and parameters
